CREATE QUERY kmeans(
  STRING v_type,
  SET<STRING> attr_set,
  STRING distance_type="EUCLIDEAN",
  INT min_cluster_count,
  INT max_cluster_count,
  INT cluster_inc=1,
  INT random_iter_count,
  INT conv_iter_limit=100,
  FLOAT conv_threshold=0.1,
  INT random_seed=42,
  BOOL use_custom_timestamp=FALSE,
  DATETIME custom_timestamp=to_datetime("1970-01-01")) SYNTAX V1
{
  TYPEDEF TUPLE<FLOAT distance, VERTEX centroid> Centroid_Distance;
  TYPEDEF TUPLE<FLOAT sse_delta, INT cluster_count, INT iter> Cluster_Count_Tuple;
  HeapAccum<Centroid_Distance> (1, distance ASC) @centroid_distance_heap;
  MaxAccum<Cluster_Count_Tuple> @@max_sse_delta;
  MapAccum<INT, MinAccum<Centroid_Distance>> @cluster_assignment;
  MapAccum<VERTEX, MapAccum<INT, AvgAccum>> @@centroid_write_values;
  MapAccum<VERTEX, ListAccum<FLOAT>> @@centroid_read_values;
  MapAccum<INT, MaxAccum<VERTEX>> @@cluster_rep_verts;

  GroupByAccum<INT cluster_count, INT iter, INT centroid_id, ListAccum<FLOAT> centroid_values> @@final_centroid_values;

  MapAccum<INT, MinAccum<FLOAT>> @@cluster_count_sse_map;

  ListAccum<FLOAT> @attr_values;
  SumAccum<FLOAT> @random_init, @@sse, @value, @speed_limit;
  AvgAccum @avg_value;
  SumAccum<INT> @random_base;
  MaxAccum<INT> @cluster_id;
  MaxAccum<FLOAT> @@max_attr_value;
  MinAccum<FLOAT> @@min_attr_value;

  DATETIME cluster_timestamp = now();
  IF use_custom_timestamp == TRUE THEN
    cluster_timestamp = custom_timestamp;
  END;

  INT _mod, _mult, _inc;
  _mod = pow(2, 31)-1;
  _mult = 1664525;
  _inc = 1013904223;

  verts = {v_type};
  INT feature_count = attr_set.size();

  MapAccum<INT, MaxAccum<STRING>> @@attr_map;

  DATETIME timestamp = now();

  JSONARRAY arr;
  FOREACH attr IN attr_set DO
    arr = parse_json_array(attr);
    @@attr_map += (arr.getInt(0) -> arr.getString(1));
  END;

  STRING attr_name;
  FOREACH attr_idx IN RANGE[0, attr_set.size()-1] DO
    attr_name = @@attr_map.get(attr_idx);
    PRINT attr_name;
    verts =
      SELECT s FROM verts:s
      ACCUM
        @@max_attr_value += s.attr_map.get(attr_name),
        @@min_attr_value += s.attr_map.get(attr_name)
      POST-ACCUM
        FLOAT denominator = @@max_attr_value - @@min_attr_value,
        IF denominator == 0 THEN
          denominator = 1
        END,
        s.@attr_values += (s.attr_map.get(attr_name)-@@min_attr_value) / denominator,
        s.@random_base = (((getvid(s)+_inc)+_mult*random_seed) % _mod),
        s.@random_init = s.@random_base / (_mod * 1.0);
    @@min_attr_value = GSQL_INT_MAX;
    @@max_attr_value = GSQL_INT_MIN;
  END;

  // if two clusters converge, merge them and just stop there.
  FLOAT merge_threshold=0.01;
  GroupByAccum<INT cluster_count, INT iter, INT centroid_id, MaxAccum<INT> merge_centroid_id> @@merge_map;
  SumAccum<INT> @@terminate_count;
  INT terminate_threshold = 5;

  FOREACH cluster_count IN RANGE[min_cluster_count, max_cluster_count].STEP(cluster_inc) DO
    IF @@terminate_count >= terminate_threshold THEN
      BREAK;
    END;
    FOREACH iter IN RANGE[0, random_iter_count-1] DO
      IF @@terminate_count >= terminate_threshold THEN
        BREAK;
      END;
      @@terminate_count = 0;
      // CENTROID INITIALIZATION
      centroids =
        SELECT s FROM verts:s
        POST-ACCUM
          s.@random_base = (s.@random_base * _mult + (getvid(s)+_inc)) % _mod,
          s.@random_init = s.@random_base / (_mod * 1.0)
        ORDER BY s.@random_init
        LIMIT cluster_count;
      centroids =
        SELECT s FROM centroids:s
        POST-ACCUM
          FOREACH i IN RANGE[0, feature_count-1] DO
            @@centroid_read_values += (s -> s.@attr_values.get(i) + s.@random_init)
          END;

      // PRINT cluster_count, iter, centroids, @@centroid_read_values;
      @@sse = 0;
      FLOAT last_sse = 1;
      BOOL first_iter = TRUE;
      // KMEANS ITERATION UNTIL COVNVERGENCE
      WHILE (abs(@@sse - last_sse) > conv_threshold OR first_iter == TRUE) AND last_sse > @@sse LIMIT conv_iter_limit DO
        first_iter = FALSE;
        last_sse = @@sse;
        @@sse = 0;
        verts =
          SELECT s FROM verts:s
          ACCUM
            FOREACH (centroid, centroid_values) IN @@centroid_read_values DO
              // compute distance
              s.@centroid_distance_heap += Centroid_Distance(tg_similarity_accum(s.@attr_values, centroid_values, distance_type), centroid)
            END
          POST-ACCUM
            FOREACH i IN RANGE[0, feature_count-1] DO
              @@centroid_write_values += (s.@centroid_distance_heap.top().centroid -> (i -> s.@attr_values.get(i)))
            END,
            @@sse += pow(s.@centroid_distance_heap.top().distance, 2);
        @@centroid_read_values.clear();
        centroids =
          SELECT s FROM centroids:s
          POST-ACCUM
            FOREACH i IN RANGE[0, feature_count-1] DO
              @@centroid_read_values += (s -> @@centroid_write_values.get(s).get(i))
            END;
        @@centroid_write_values.clear();
      END;
      // CLEAN UP
      centroids =
        SELECT s FROM centroids:s
        POST-ACCUM
          // compare against other centroids
          // if too close, and self less than other cluster id, add to merge map
          // for now, even a single overlap results in termination, in the future, we might want to add a threshold for X overlaps
          BOOL terminate = FALSE,
          FOREACH (centroid, centroid_values) IN @@centroid_read_values DO
            IF
                centroid != s AND
                getvid(s) < getvid(centroid) AND
                tg_similarity_accum(@@centroid_read_values.get(s), centroid_values, distance_type) < merge_threshold
                THEN
              @@merge_map += (cluster_count, iter, getvid(s) -> getvid(centroid)),
              @@terminate_count += 1,
              terminate = TRUE,
              BREAK
            END
          END,
          IF terminate == FALSE THEN
            @@final_centroid_values += (cluster_count, iter, getvid(s) -> @@centroid_read_values.get(s))
          END;
      @@centroid_read_values.clear();
      verts =
        SELECT s FROM verts:s
        POST-ACCUM
          s.@cluster_assignment += (cluster_count -> Centroid_Distance(@@sse, s.@centroid_distance_heap.top().centroid)),
          s.@centroid_distance_heap.clear();
      @@cluster_count_sse_map += (cluster_count -> @@sse);

      IF (cluster_count - min_cluster_count) > 2*cluster_inc THEN
        FLOAT prev_sse_delta = @@cluster_count_sse_map.get(cluster_count-(2*cluster_inc)) - @@cluster_count_sse_map.get(cluster_count-(1*cluster_inc));
        FLOAT next_sse_delta = @@cluster_count_sse_map.get(cluster_count-(1*cluster_inc)) - @@cluster_count_sse_map.get(cluster_count);
        @@max_sse_delta += Cluster_Count_Tuple(next_sse_delta - prev_sse_delta, cluster_count-(1*cluster_inc), iter);
      END;
    END;
  END;

  rep_verts =
    SELECT s FROM verts:s
    ACCUM
      INT cluster_id = getvid(s.@cluster_assignment.get(@@max_sse_delta.cluster_count).centroid),
      IF @@merge_map.containsKey(
      @@max_sse_delta.cluster_count,
      @@max_sse_delta.iter,
      cluster_id) THEN
        cluster_id = @@merge_map.get(@@max_sse_delta.cluster_count, @@max_sse_delta.iter,cluster_id).merge_centroid_id
      END,
      s.@cluster_id = cluster_id,
      @@cluster_rep_verts += (cluster_id -> s)
    POST-ACCUM
      INSERT INTO In_Cluster VALUES (s, s.@cluster_id, cluster_timestamp)
    HAVING
      @@cluster_rep_verts.get(s.@cluster_id) == s;

  rep_verts =
    SELECT s FROM rep_verts:s
    POST-ACCUM
      INSERT INTO Cluster VALUES (s.@cluster_id, @@final_centroid_values.get(@@max_sse_delta.cluster_count, @@max_sse_delta.iter, s.@cluster_id).centroid_values);

  PRINT "Created Cluster Count:", rep_verts.size();
}

CREATE QUERY combine_features(
  SET<STRING> hub_v_type,
  SET<STRING> e_type,
  STRING target_v_type,
  SET<STRING> feature_v_type,
  INT hub_threshold,
  INT split_threshold) SYNTAX V1
{
  MapAccum<VERTEX, SumAccum<FLOAT>> @intersection_size_map;
  OrAccum @hub;
  DATETIME timestamp = now();

  hub_verts = {hub_v_type};
  hub_verts =
    SELECT s FROM hub_verts:s
    WHERE s.outdegree(e_type) > hub_threshold
    POST-ACCUM s.@hub += TRUE;

  hub_verts =
    SELECT s FROM hub_verts:s -(e_type)- target_v_type -(e_type)- feature_v_type:t
    WHERE t.@hub == FALSE OR getvid(s) > getvid(t)
    ACCUM
      s.@intersection_size_map += (t -> 1);


  // This section of the code cannot be made schema-free
  hub_verts =
    SELECT s FROM hub_verts:s -(e_type)- target_v_type:v -(e_type)- feature_v_type:t
    WHERE (t.@hub == FALSE OR getvid(s) > getvid(t)) AND s.@intersection_size_map.get(t) BETWEEN 2 AND split_threshold
    ACCUM
      INSERT INTO Has_Attribute VALUES (v, to_string(getvid(s)) + "_" + to_string(getvid(t)) Combined_Feature, timestamp, 1);

  hub_verts =
    SELECT s FROM hub_verts:s -(e_type)- target_v_type -(e_type)- feature_v_type:t
    WHERE (t.@hub == FALSE OR getvid(s) > getvid(t)) AND s.@intersection_size_map.get(t) BETWEEN 2 AND split_threshold
    PER (s, t)
    ACCUM
      INSERT INTO Linked VALUES (s, to_string(getvid(s)) + "_" + to_string(getvid(t)) Combined_Feature, timestamp),
      INSERT INTO Linked VALUES (t, to_string(getvid(s)) + "_" + to_string(getvid(t)) Combined_Feature, timestamp);
}


CREATE DISTRIBUTED QUERY recommend_products(
  INT batch_index,
  INT num_batches,
  INT target_batch=5,
  FLOAT ignore_threshold = 0.001,
  INT recommendation_count,
  STRING data_types = "[[\"Customer\",\"Feature\",\"Edge\",[\"Interacted\"]], [\"Customer\",\"Feature\",\"Vertex\",[\"Product_Variant\"]], [\"Customer\",\"Target\",\"Edge\",[\"Interacted\"]], [\"Customer\",\"Target\",\"Vertex\",[\"Customer\"]], [\"Item\",\"Feature\",\"Edge\",[\"Interacted\"]], [\"Item\",\"Feature\",\"Vertex\",[\"Customer\"]], [\"Item\",\"Target\",\"Edge\",[\"Interacted\"]], [\"Item\",\"Target\",\"Vertex\",[\"Product_Variant\"]]]",
  STRING edge_importance_factors = "[\"Customer\", \"Interacted\", 1], [\"Item\", \"Interacted\", 1]",
  SET<STRING> edge_degree_scales,
  FLOAT customer_popularity_scale = 1,
  FLOAT item_popularity_scale = 1,
  BOOL print_results = TRUE) SYNTAX V1
{
  TYPEDEF TUPLE<FLOAT score, VERTEX item> Item_Tuple;
  HeapAccum<Item_Tuple> (recommendation_count, score DESC, item DESC) @recommended_items;
  MapAccum<VERTEX, SumAccum<INT>> @sum_intersection_size, @@sum_set_size;
  MapAccum<VERTEX, MapAccum<VERTEX, SumAccum<FLOAT>>> @item_intersect_map;
  MapAccum<VERTEX, SumAccum<FLOAT>> @sum_similarity, @popularity, @item_popularity;
  MaxAccum<FLOAT> @category_importance;
  SetAccum<VERTEX> @ignore, @visited, @temp_set;
  OrAccum @source_vert;

  MapAccum<VERTEX, MaxAccum<FLOAT>> @max_similarity;
  MapAccum<STRING, MaxAccum<FLOAT>> @@max_value;
  MapAccum<STRING, MinAccum<FLOAT>> @@min_value;

  GroupByAccum<STRING similarity_type, STRING overlap_type, STRING data_type, SetAccum<STRING> types> @@data_types;
  GroupByAccum<STRING similarity_type, STRING edge_type, MaxAccum<FLOAT> scale_factor> @@importance_factors;
  SetAccum<STRING> @@edge_degree_scales;

  JSONARRAY arr = parse_json_array(data_types);
  JSONARRAY inner_arr;
  JSONARRAY entry;
  FOREACH i IN RANGE[0, arr.size()-1] DO
    entry = arr.getJSONArray(i);
    // Customer/Item, Feature/Target, Edge/Vertex -> Type
    inner_arr = entry.getJSONArray(3);
    FOREACH j IN RANGE[0, inner_arr.size()-1] DO
      @@data_types += (entry.getString(0), entry.getString(1), entry.getString(2) -> inner_arr.getString(j));
      IF @@edge_degree_scales.size() == 0 AND entry.getString(2) == "Edge" THEN
        @@edge_degree_scales += entry.getString(3);
      END;
    END;
  END;

  arr = parse_json_array(edge_importance_factors);
  FOREACH i IN RANGE[0, arr.size()-1] DO
    // Customer/Item/Category, Type/Id -> Factor
    entry = arr.getJSONArray(i);
    @@importance_factors += (entry.getString(0), entry.getString(1) -> entry.getDouble(2));
  END;

  all_customers (ANY) = {Customer.*};

  src_customers =
    SELECT s FROM all_customers:s
    WHERE getvid(s) % num_batches == batch_index
    POST-ACCUM
      s.@source_vert += TRUE;


  // customer similarity based on purchase history and cluster
  previous_items =
    SELECT t
    FROM src_customers:s-(Interacted:e)-Product_Variant:t
    WHERE e.interaction_type == "Purchased"
    ACCUM t.@temp_set += s;

  previous_items =
    SELECT t FROM previous_items:s -(Belongs_To)- Product:t
    ACCUM
      t.@temp_set += s.@temp_set
    POST-ACCUM
      s.@temp_set.clear();

  previous_items =
    SELECT t FROM previous_items:s -(reverse_Belongs_To)- Product_Variant:t
    ACCUM
      t.@ignore += s.@temp_set
    POST-ACCUM
      s.@temp_set.clear();

  // get all verts for which we need to calculate set size
  SetAccum<STRING> @@e_types, @@v_types;
  @@e_types = @@data_types.get("Customer", "Feature", "Edge").types;
  @@v_types = @@data_types.get("Customer", "Feature", "Vertex").types;
  features =
    SELECT t FROM src_customers:s -(@@e_types)- @@v_types:t;

  @@e_types = @@data_types.get("Customer", "Target", "Edge").types;
  @@v_types = @@data_types.get("Customer", "Target", "Vertex").types;
  other_customers =
    SELECT t FROM features:s -(@@e_types)- @@v_types:t;

  included_customers = other_customers UNION src_customers;

  // scale importance of features by degree
  // ideally this is a cached property
  // calculate set sizes for everyone
  @@e_types = @@data_types.get("Customer", "Feature", "Edge").types;
  @@v_types = @@data_types.get("Customer", "Feature", "Vertex").types;
  inluded_customers =
    SELECT s FROM included_customers:s -(@@e_types:e)- @@v_types:t
    WHERE @@edge_degree_scales.contains(e.type)
    ACCUM
      @@max_value += (t.type -> t.outdegree(@@data_types.get("Customer", "Target", "Edge").types)),
      @@min_value += (t.type -> t.outdegree(@@data_types.get("Customer", "Target", "Edge").types));

  @@e_types = @@data_types.get("Customer", "Feature", "Edge").types;
  @@v_types = @@data_types.get("Customer", "Feature", "Vertex").types;
  features =
    SELECT t FROM included_customers:s -(@@e_types:e)- @@v_types:t
    ACCUM
      FLOAT sim_score = 1,
      INT deg = t.outdegree(@@data_types.get("Customer", "Target", "Edge").types),
      IF e.type == "Interacted" THEN
        sim_score = e.score
      END,
      IF @@importance_factors.containsKey("Customer", e.type) THEN
        sim_score = sim_score * @@importance_factors.get("Customer", e.type).scale_factor
      END,
      IF @@edge_degree_scales.contains(e.type) THEN
        sim_score = sim_score * max([1-((deg - @@min_value.get(t.type)) / (@@max_value.get(t.type) - @@min_value.get(t.type))), 0.1])
      END,
      @@sum_set_size += (s -> sim_score),
      IF s.@source_vert == TRUE THEN
        t.@sum_similarity += (s -> sim_score)
        END;

  @@e_types = @@data_types.get("Customer", "Target", "Edge").types;
  @@v_types = @@data_types.get("Customer", "Target", "Vertex").types;
  other_customers =
    SELECT t
    FROM features:s -(@@e_types:e)- @@v_types:t
    ACCUM
      t.@sum_intersection_size += s.@sum_similarity
    POST-ACCUM (t)
      FOREACH (tgt, score) IN t.@sum_intersection_size DO
        FLOAT div = (@@sum_set_size.get(t) + @@sum_set_size.get(tgt) - score),
        IF div <= 0 THEN
          div = 1.0
        END,
        t.@sum_similarity += (tgt -> score*1.0/div)
      END,
      t.@sum_intersection_size.clear()
    POST-ACCUM (s)
      s.@sum_similarity.clear(),
      s.@sum_intersection_size.clear();

  item_links = features UNION other_customers;

  // calculate which items are most popular/compatible
  items =
    SELECT t FROM item_links:s -(Has_Attribute|Interacted)- Product_Variant:t
    ACCUM
      FOREACH (src, score) IN s.@sum_similarity DO
        IF s.@ignore.contains(src) == FALSE THEN
          t.@popularity += (src -> score)
        END
      END
    POST-ACCUM (t)
      FOREACH (src, score) IN t.@popularity DO
        //t.@visited += src,
        IF score <= ignore_threshold THEN
          t.@ignore += src
        END
      END;

  @@sum_set_size.clear();
  item_links =
    SELECT s FROM item_links:s
    POST-ACCUM
      s.@sum_intersection_size.clear(),
      s.@sum_similarity.clear();

  // traverse from kept items to categories and update parent map
  // traverse via item attributes to other items
  // consider including customer purchases and box history
  // update popularity of item

  // traverse to previously interacted items
  interacted_items =
    SELECT t FROM src_customers:s -(Interacted:e)- Product_Variant:t
    ACCUM t.@max_similarity += (s -> e.score);

  // get parents of interacted item variants
  parent_items =
    SELECT t FROM interacted_items:s -(Belongs_To)- Product:t
    ACCUM
      t.@max_similarity += s.@max_similarity;

  interacted_items =
    SELECT t FROM parent_items:s -(reverse_Belongs_To)- Product_Variant:t
    ACCUM
      t.@max_similarity += s.@max_similarity
    POST-ACCUM
      t.@source_vert += TRUE;

  ////////////////
  // calculate range of values of neighborhood set sizes
  @@max_value.clear();
  @@min_value.clear();

  @@e_types = @@data_types.get("Item", "Feature", "Edge").types;
  @@v_types = @@data_types.get("Item", "Feature", "Vertex").types;
  customers (ANY) =
    SELECT t FROM interacted_items:s -(@@e_types)- @@v_types:t;

  @@e_types = @@data_types.get("Item", "Target", "Edge").types;
  @@v_types = @@data_types.get("Item", "Target", "Vertex").types;
  other_items =
    SELECT t FROM customers:s -(@@e_types)- @@v_types:t;

  included_items = other_items UNION interacted_items;

  @@e_types = @@data_types.get("Item", "Feature", "Edge").types;
  @@v_types = @@data_types.get("Item", "Feature", "Vertex").types;
  inluded_items =
    SELECT s FROM included_items:s -(@@e_types:e)- @@v_types:t
    WHERE @@edge_degree_scales.contains(e.type)
    ACCUM
      @@max_value += (t.type -> t.outdegree(@@data_types.get("Item", "Target", "Edge").types)),
      @@min_value += (t.type -> t.outdegree(@@data_types.get("Item", "Target", "Edge").types));

  // calculate overlap on customers
  FOREACH i IN RANGE[0, target_batch-1] DO
    @@e_types = @@data_types.get("Item", "Feature", "Edge").types;
    @@v_types = @@data_types.get("Item", "Feature", "Vertex").types;
    customers =
      SELECT t FROM included_items:s -(@@e_types:e)- @@v_types:t
      WHERE getvid(t) % target_batch == i
      ACCUM
        FLOAT sim_score = 1,
        INT deg = t.outdegree(@@data_types.get("Item", "Target", "Edge").types),
        IF e.type == "Interacted" THEN
          sim_score = e.score
        END,
        IF @@importance_factors.containsKey("Item", e.type) THEN
          sim_score = sim_score * @@importance_factors.get("Item", e.type).scale_factor
        END,
        IF @@edge_degree_scales.contains(e.type) THEN
          sim_score = sim_score * max([1-((deg - @@min_value.get(t.type)) / (@@max_value.get(t.type) - @@min_value.get(t.type))), 0.1])
        END,
        // calculate set size of source item
        @@sum_set_size += (s -> sim_score),
        // scale overlap by contribution from source customer
        FOREACH (src_customer, score) IN s.@max_similarity DO
          IF src_customer == t THEN
            CONTINUE
          END,
          IF s.@source_vert == TRUE THEN
            t.@item_intersect_map += (src_customer -> (s -> sim_score * score))
          END
        END;
    // traverse to other items...
    @@e_types = @@data_types.get("Item", "Target", "Edge").types;
    @@v_types = @@data_types.get("Item", "Target", "Vertex").types;
    similar_items =
      SELECT t
      FROM customers:s -(@@e_types:e)- @@v_types:t
      ACCUM
        t.@item_intersect_map += s.@item_intersect_map
      POST-ACCUM (t)
        FOREACH (src_customer, score_map) IN t.@item_intersect_map DO
          IF t.@ignore.contains(src_customer) THEN
            CONTINUE
          END,
          FOREACH (src_item, score) IN score_map DO
            FLOAT div = (@@sum_set_size.get(t) + @@sum_set_size.get(src_item) - score),
            IF div <= 0 THEN
              div = 1
            END,
            t.@item_popularity += (src_customer -> score*1.0/div)
          END
        END,
        t.@item_intersect_map.clear()
      POST-ACCUM (s)
        s.@item_intersect_map.clear();
  END;

  /////////////////
  items = items UNION similar_items;

  // apply category importance scale
  item_categories (Product_Category) = {Product_Category.*};
  categories (ANY) =
    SELECT s FROM item_categories:s WHERE @@importance_factors.containsKey("Category", s.id)
    POST-ACCUM s.@category_importance = @@importance_factors.get("Category", s.id).scale_factor;

  WHILE categories.size() > 0 DO
    categories =
      SELECT t FROM categories:s -(reverse_Belongs_To)- (Product_Category|Product|Product_Variant):t
      WHERE
        (t.type != "Product_Variant" OR t.@visited.size() > 0)
      ACCUM
        t.@category_importance += s.@category_importance;
  END;

  // INTERLEAVE RECOMMENDATIONS
  GroupByAccum<STRING popularity_type, VERTEX src_customer, MaxAccum<FLOAT> max_value> @@popularity_max_value;
  // final filter + normalization
  items =
    SELECT s FROM items:s
    POST-ACCUM
      FLOAT popularity = 0,
      FOREACH (src_customer, score) IN s.@popularity DO
        popularity = score * s.@category_importance * customer_popularity_scale,
        @@popularity_max_value += ("popularity", src_customer -> popularity)
      END,
      FOREACH (src_customer, score) IN s.@item_popularity DO
        popularity = score * s.@category_importance * item_popularity_scale,
        @@popularity_max_value += ("item_popularity", src_customer -> popularity)
      END;

  items =
    SELECT s FROM items:s
    ACCUM
      FLOAT popularity = 0,
      FOREACH (src_customer, score) IN s.@popularity DO
        IF @@popularity_max_value.get("popularity", src_customer).max_value > 0 THEN
          popularity = score * s.@category_importance * customer_popularity_scale,
          src_customer.@recommended_items += Item_Tuple(popularity / @@popularity_max_value.get("popularity", src_customer).max_value, s)
        END
      END,
      FOREACH (src_customer, score) IN s.@item_popularity DO
        popularity = score * s.@category_importance * item_popularity_scale,
        src_customer.@recommended_items += Item_Tuple(popularity / @@popularity_max_value.get("item_popularity", src_customer).max_value, s)
      END;

  IF print_results THEN
    PRINT src_customers[src_customers.@recommended_items];
  END;
}
